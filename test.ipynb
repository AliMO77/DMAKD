{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "X_train, X_valtest, y_train, y_valtest = train_test_split(X, y, test_size=0.2, random_state=43)\n",
    "\n",
    "# Split the combined validation/test set into separate validation and test sets\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_valtest, y_valtest, test_size=0.5, random_state=43)\n",
    "\n",
    "\n",
    "def my_FNN(hidden_layers, hidden_layer_dim, activation,learning_rate, batch_size, epochs):\n",
    "    # Define the model architecture\n",
    "    model = tf.keras.Sequential()\n",
    "    \n",
    "    \n",
    "    model.add(tf.keras.layers.Dense(hidden_layer_dim, activation=activation, input_shape=(X_train.shape[1],)))\n",
    "    model.add(tf.keras.layers.Dropout(0.2))\n",
    "\n",
    "\n",
    "    for i in range(hidden_layers-1):\n",
    "            model.add(tf.keras.layers.Dense(hidden_layer_dim, activation=activation))\n",
    "            model.add(tf.keras.layers.Dropout(0.2))\n",
    "\n",
    "    model.add(tf.keras.layers.Dense(1, activation='sigmoid'))\n",
    "   \n",
    "    # Compile the model\n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate),\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "    # Train the model on the training set and validate on the validation set\n",
    "    model.fit(X_train, y_train, batch_size,epochs, validation_data=(X_val, y_val))\n",
    "\n",
    "    # Evaluate the model on the test set\n",
    "    test_loss, test_acc = model.evaluate(X_test, y_test)\n",
    "    print('Test accuracy:', test_acc)\n",
    "    return model \n",
    "\n",
    "my_FNN(12,1024,'relu',0.001,512,150)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "param_space = {\n",
    "    \"hidden_layers\": [2, 3, 4],\n",
    "    \"hidden_layer_dim\": [64, 128, 256],\n",
    "    \"activation\": [\"relu\", \"sigmoid\"],\n",
    "    \"optimizer\": [\"adam\", \"sgd\"],\n",
    "    \"learning_rate\": [0.01, 0.001, 0.0001],\n",
    "    \"batch_size\": [128, 256, 512],\n",
    "    \"epochs\": [100, 150, 200],\n",
    "}\n",
    "def fitness(params,X_train, y_train, X_val, y_val):\n",
    "    # Create the model\n",
    "    model = KerasClassifier(my_FNN, **params)\n",
    "    # Train the model on the training data\n",
    "    model.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=params[\"epochs\"], batch_size=params[\"batch_size\"])\n",
    "    # Evaluate the model on the validation set\n",
    "    score = model.score(X_val, y_val)\n",
    "    # Return the accuracy as the fitness\n",
    "    return score\n",
    "\n",
    "def evaluate_individual(individual, X_train, y_train, X_val, y_val, X_test, y_test):\n",
    "    print(type(individual))\n",
    "    params = {\n",
    "        \"hidden_layers\": individual[0],\n",
    "        \"hidden_layer_dim\": individual[1],\n",
    "        \"activation\": individual[2],\n",
    "        \"optimizer\": individual[3],\n",
    "        \"learning_rate\": individual[4],\n",
    "        \"batch_size\": individual[5],\n",
    "        \"epochs\": individual[6]\n",
    "    }\n",
    "    accuracy = fitness(params, X_train, y_train, X_val, y_val)\n",
    "   \n",
    "    return accuracy,\n",
    "\n",
    "# Define the genetic operators\n",
    "creator.create(\"FitnessMax\", base.Fitness, weights=(1.0,))\n",
    "creator.create(\"Individual\", list, fitness=creator.FitnessMax)\n",
    "\n",
    "toolbox = base.Toolbox()\n",
    "\n",
    "def init_individual(toolbox):\n",
    "    return (toolbox.hidden_layers(),\n",
    "            toolbox.hidden_layer_dim(),\n",
    "            toolbox.activation(),\n",
    "            toolbox.optimizer(),\n",
    "            toolbox.learning_rate(),\n",
    "            toolbox.batch_size(),\n",
    "            toolbox.epochs())\n",
    "\n",
    "\n",
    "\n",
    "toolbox.register(\"hidden_layers\", random.choice, param_space[\"hidden_layers\"])\n",
    "toolbox.register(\"hidden_layer_dim\", random.choice, param_space[\"hidden_layer_dim\"])\n",
    "toolbox.register(\"activation\", random.choice, param_space[\"activation\"])\n",
    "toolbox.register(\"optimizer\", random.choice, param_space[\"optimizer\"])\n",
    "toolbox.register(\"learning_rate\", random.choice, param_space[\"learning_rate\"])\n",
    "toolbox.register(\"batch_size\", random.choice, param_space[\"batch_size\"])\n",
    "toolbox.register(\"epochs\", random.choice, param_space[\"epochs\"])\n",
    "\n",
    "toolbox.register(\"individual\", tools.initIterate, creator.Individual, \n",
    "                 functools.partial(init_individual, toolbox))\n",
    "\n",
    "\n",
    "toolbox.register(\"population\", tools.initRepeat, list, toolbox.individual)\n",
    "\n",
    "toolbox.register(\"evaluate\", evaluate_individual, X_train=X_train, y_train=y_train, X_val=X_val, y_val=y_val, X_test=X_test, y_test=y_test)\n",
    "toolbox.register(\"mate\", tools.cxUniform, indpb=0.1)\n",
    "\n",
    "toolbox.register(\"mutate\", tools.mutUniformInt, low=0, up=len(param_space[\"optimizer\"]) - 1, indpb=0.1)\n",
    "\n",
    "toolbox.register(\"select\", tools.selTournament, tournsize=3)\n",
    "\n",
    "population = toolbox.population(n=100)\n",
    "\n",
    "NGEN = 10\n",
    "\n",
    "\n",
    "best_fitness = -float('inf')\n",
    "num_generations_without_improvement = 0\n",
    "MAX_GENERATIONS_WITHOUT_IMPROVEMENT = 3\n",
    "for gen in range(NGEN):\n",
    "    offspring = algorithms.varAnd(population, toolbox, cxpb=0.5, mutpb=0.2)\n",
    "    fits = list(toolbox.map(toolbox.evaluate, offspring))    \n",
    "    for fit, ind in zip(fits, offspring):\n",
    "        ind.fitness.values = (fit,)\n",
    "    \n",
    "    population = toolbox.select(offspring, k=len(population))\n",
    "    top1 = tools.selBest(population, k=1)\n",
    "    current_fitness = top1[0].fitness.values[0]\n",
    "    print(\"Generation %i, Top 1 accuracy %f\" % (gen, current_fitness))\n",
    "    \n",
    "    if current_fitness > best_fitness:\n",
    "        best_fitness = current_fitness\n",
    "        num_generations_without_improvement = 0\n",
    "    else:\n",
    "        num_generations_without_improvement += 1\n",
    "        if num_generations_without_improvement >= MAX_GENERATIONS_WITHOUT_IMPROVEMENT:\n",
    "            print(\"Stopping evolution process because there have been no improvements in the last %d generations.\" % MAX_GENERATIONS_WITHOUT_IMPROVEMENT)\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.svm import SVC\n",
    "from scipy.stats import uniform, randint\n",
    "\n",
    "# Define the parameter space for hyperparameters\n",
    "param_dist = {'C': uniform(0.1, 10),\n",
    "              'gamma': uniform(0.1, 1),\n",
    "              'kernel': ['rbf', 'linear', 'poly']}\n",
    "\n",
    "# Create an SVM object\n",
    "svc = SVC()\n",
    "\n",
    "# Define the RandomizedSearchCV object with 10-fold cross-validation and 20 iterations\n",
    "random_search = RandomizedSearchCV(estimator=svc,\n",
    "                                   param_distributions=param_dist,\n",
    "                                   n_iter=20,\n",
    "                                   cv=10,\n",
    "                                   n_jobs=-1)\n",
    "\n",
    "# Fit the model to the data\n",
    "random_search.fit(X_train, y_train)\n",
    "\n",
    "# Print the best hyperparameters and accuracy score\n",
    "print(\"Best hyperparameters: \", random_search.best_params_)\n",
    "print(\"Best accuracy score: \", random_search.best_score_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Decision Tree': 90, 'Logistic regression': 91, 'Randomn forrest': 91, 'XGBoost': 91}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "models = ['Logistic regression','Decision Tree', 'Randomn forrest','XGBoost']\n",
    "accuracies = [91,90,91,91]\n",
    "\n",
    "result_dict = {key:value for key,value in zip(models, accuracies)}\n",
    "sorted_dict = dict(sorted(result_dict.items(), key=lambda x: x[1]))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Shape of passed values is (5, 2), indices imply (5, 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/pandas/core/internals/managers.py:1662\u001b[0m, in \u001b[0;36mcreate_block_manager_from_blocks\u001b[0;34m(blocks, axes)\u001b[0m\n\u001b[1;32m   1657\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1658\u001b[0m         \u001b[39m# It's OK if a single block is passed as values, its placement\u001b[39;00m\n\u001b[1;32m   1659\u001b[0m         \u001b[39m# is basically \"all items\", but if there're many, don't bother\u001b[39;00m\n\u001b[1;32m   1660\u001b[0m         \u001b[39m# converting, it's an error anyway.\u001b[39;00m\n\u001b[1;32m   1661\u001b[0m         blocks \u001b[39m=\u001b[39m [\n\u001b[0;32m-> 1662\u001b[0m             make_block(values\u001b[39m=\u001b[39;49mblocks[\u001b[39m0\u001b[39;49m], placement\u001b[39m=\u001b[39;49m\u001b[39mslice\u001b[39;49m(\u001b[39m0\u001b[39;49m, \u001b[39mlen\u001b[39;49m(axes[\u001b[39m0\u001b[39;49m])))\n\u001b[1;32m   1663\u001b[0m         ]\n\u001b[1;32m   1665\u001b[0m mgr \u001b[39m=\u001b[39m BlockManager(blocks, axes)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/pandas/core/internals/blocks.py:2722\u001b[0m, in \u001b[0;36mmake_block\u001b[0;34m(values, placement, klass, ndim, dtype)\u001b[0m\n\u001b[1;32m   2720\u001b[0m     values \u001b[39m=\u001b[39m DatetimeArray\u001b[39m.\u001b[39m_simple_new(values, dtype\u001b[39m=\u001b[39mdtype)\n\u001b[0;32m-> 2722\u001b[0m \u001b[39mreturn\u001b[39;00m klass(values, ndim\u001b[39m=\u001b[39;49mndim, placement\u001b[39m=\u001b[39;49mplacement)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/pandas/core/internals/blocks.py:130\u001b[0m, in \u001b[0;36mBlock.__init__\u001b[0;34m(self, values, placement, ndim)\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_validate_ndim \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mndim \u001b[39mand\u001b[39;00m \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmgr_locs) \u001b[39m!=\u001b[39m \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mvalues):\n\u001b[0;32m--> 130\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    131\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mWrong number of items passed \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mvalues)\u001b[39m}\u001b[39;00m\u001b[39m, \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    132\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mplacement implies \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmgr_locs)\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    133\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: Wrong number of items passed 2, placement implies 1",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 18\u001b[0m\n\u001b[1;32m     15\u001b[0m transformed_data \u001b[38;5;241m=\u001b[39m pca\u001b[38;5;241m.\u001b[39mtransform(df[[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mColumn 1\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mColumn 2\u001b[39m\u001b[38;5;124m'\u001b[39m]])\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# Create a new DataFrame with the transformed data\u001b[39;00m\n\u001b[0;32m---> 18\u001b[0m df_transformed \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDataFrame\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtransformed_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mPC1\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOriginal DataFrame:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28mprint\u001b[39m(df)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/pandas/core/frame.py:497\u001b[0m, in \u001b[0;36mDataFrame.__init__\u001b[0;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[1;32m    495\u001b[0m         mgr \u001b[39m=\u001b[39m init_dict({data\u001b[39m.\u001b[39mname: data}, index, columns, dtype\u001b[39m=\u001b[39mdtype)\n\u001b[1;32m    496\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 497\u001b[0m         mgr \u001b[39m=\u001b[39m init_ndarray(data, index, columns, dtype\u001b[39m=\u001b[39;49mdtype, copy\u001b[39m=\u001b[39;49mcopy)\n\u001b[1;32m    499\u001b[0m \u001b[39m# For data is list-like, or Iterable (will consume into list)\u001b[39;00m\n\u001b[1;32m    500\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(data, abc\u001b[39m.\u001b[39mIterable) \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(data, (\u001b[39mstr\u001b[39m, \u001b[39mbytes\u001b[39m)):\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/pandas/core/internals/construction.py:234\u001b[0m, in \u001b[0;36minit_ndarray\u001b[0;34m(values, index, columns, dtype, copy)\u001b[0m\n\u001b[1;32m    231\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    232\u001b[0m     block_values \u001b[39m=\u001b[39m [values]\n\u001b[0;32m--> 234\u001b[0m \u001b[39mreturn\u001b[39;00m create_block_manager_from_blocks(block_values, [columns, index])\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/pandas/core/internals/managers.py:1672\u001b[0m, in \u001b[0;36mcreate_block_manager_from_blocks\u001b[0;34m(blocks, axes)\u001b[0m\n\u001b[1;32m   1670\u001b[0m blocks \u001b[39m=\u001b[39m [\u001b[39mgetattr\u001b[39m(b, \u001b[39m\"\u001b[39m\u001b[39mvalues\u001b[39m\u001b[39m\"\u001b[39m, b) \u001b[39mfor\u001b[39;00m b \u001b[39min\u001b[39;00m blocks]\n\u001b[1;32m   1671\u001b[0m tot_items \u001b[39m=\u001b[39m \u001b[39msum\u001b[39m(b\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m] \u001b[39mfor\u001b[39;00m b \u001b[39min\u001b[39;00m blocks)\n\u001b[0;32m-> 1672\u001b[0m \u001b[39mraise\u001b[39;00m construction_error(tot_items, blocks[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m:], axes, e)\n",
      "\u001b[0;31mValueError\u001b[0m: Shape of passed values is (5, 2), indices imply (5, 1)"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Create a sample DataFrame with two correlated columns\n",
    "data = {'Column 1': [1, 2, 3, 4, 5], 'Column 2': [1, 2, 3, 4, 5]}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Initialize PCA\n",
    "pca = PCA(n_components=1)  # Set the desired number of components\n",
    "\n",
    "# Fit PCA to the correlated columns\n",
    "pca.fit(df[['Column 1', 'Column 2']])\n",
    "\n",
    "# Transform the data using the fitted PCA\n",
    "transformed_data = pca.transform(df[['Column 1', 'Column 2']])\n",
    "\n",
    "# Create a new DataFrame with the transformed data\n",
    "df_transformed = pd.DataFrame(transformed_data, columns=['PC1'])\n",
    "\n",
    "print(\"Original DataFrame:\")\n",
    "print(df)\n",
    "print(\"\\nTransformed DataFrame:\")\n",
    "print(df_transformed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
